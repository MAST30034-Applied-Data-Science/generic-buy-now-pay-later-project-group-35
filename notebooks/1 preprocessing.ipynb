{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T02:22:01.919813Z",
     "start_time": "2022-07-31T02:21:57.283647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 00:27:36 WARN Utils: Your hostname, AryansLaptop resolves to a loopback address: 127.0.1.1; using 172.28.42.139 instead (on interface eth0)\n",
      "22/09/17 00:27:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/17 00:27:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th></tr>\n",
       "<tr><td>18478</td><td>62191208634</td><td>63.255848959735246</td><td>949a63c8-29f7-4ab...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>2</td><td>15549624934</td><td>130.3505283105634</td><td>6a84c3cf-612a-457...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>18479</td><td>64403598239</td><td>120.15860593212783</td><td>b10dcc33-e53f-425...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>3</td><td>60956456424</td><td>136.6785200286976</td><td>0f09c5a5-784e-447...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>18479</td><td>94493496784</td><td>72.96316578355305</td><td>f6c78c1a-4600-4c5...</td><td>2021-08-20</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|\n",
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|  18478| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|\n",
       "|      2| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|\n",
       "|  18479| 64403598239|120.15860593212783|b10dcc33-e53f-425...|    2021-08-20|\n",
       "|      3| 60956456424| 136.6785200286976|0f09c5a5-784e-447...|    2021-08-20|\n",
       "|  18479| 94493496784| 72.96316578355305|f6c78c1a-4600-4c5...|    2021-08-20|\n",
       "+-------+------------+------------------+--------------------+--------------+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactiondf1 = spark.read.parquet(\"../data/tables/transactions_20210228_20210827_snapshot/\")\n",
    "transactiondf1.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th></tr>\n",
       "<tr><td>14935</td><td>79417999332</td><td>136.06570809815838</td><td>23acbb7b-cf98-458...</td><td>2021-11-26</td></tr>\n",
       "<tr><td>1</td><td>46451548968</td><td>72.61581642788431</td><td>76bab304-fa2d-400...</td><td>2021-11-26</td></tr>\n",
       "<tr><td>14936</td><td>89518629617</td><td>3.0783487174439297</td><td>a2ae446a-2959-41c...</td><td>2021-11-26</td></tr>\n",
       "<tr><td>1</td><td>49167531725</td><td>51.58228625503599</td><td>7080c274-17f7-4cc...</td><td>2021-11-26</td></tr>\n",
       "<tr><td>14936</td><td>31101120643</td><td>25.228114942417797</td><td>8e301c0f-06ab-45c...</td><td>2021-11-26</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|\n",
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|  14935| 79417999332|136.06570809815838|23acbb7b-cf98-458...|    2021-11-26|\n",
       "|      1| 46451548968| 72.61581642788431|76bab304-fa2d-400...|    2021-11-26|\n",
       "|  14936| 89518629617|3.0783487174439297|a2ae446a-2959-41c...|    2021-11-26|\n",
       "|      1| 49167531725| 51.58228625503599|7080c274-17f7-4cc...|    2021-11-26|\n",
       "|  14936| 31101120643|25.228114942417797|8e301c0f-06ab-45c...|    2021-11-26|\n",
       "+-------+------------+------------------+--------------------+--------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactiondf2 = spark.read.parquet(\"../data/tables/transactions_20210828_20220227_snapshot/\")\n",
    "transactiondf2.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>consumer_id</th></tr>\n",
       "<tr><td>1</td><td>1195503</td></tr>\n",
       "<tr><td>2</td><td>179208</td></tr>\n",
       "<tr><td>3</td><td>1194530</td></tr>\n",
       "<tr><td>4</td><td>154128</td></tr>\n",
       "<tr><td>5</td><td>712975</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------+\n",
       "|user_id|consumer_id|\n",
       "+-------+-----------+\n",
       "|      1|    1195503|\n",
       "|      2|     179208|\n",
       "|      3|    1194530|\n",
       "|      4|     154128|\n",
       "|      5|     712975|\n",
       "+-------+-----------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf = spark.read.parquet(\"../data/tables/consumer_user_details.parquet\")\n",
    "userdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_name</th><th>address</th><th>state</th><th>postcode</th><th>gender</th><th>consumer_id</th></tr>\n",
       "<tr><td>Yolanda Williams</td><td>413 Haney Gardens...</td><td>WA</td><td>6935</td><td>Female</td><td>1195503</td></tr>\n",
       "<tr><td>Mary Smith</td><td>3764 Amber Oval</td><td>NSW</td><td>2782</td><td>Female</td><td>179208</td></tr>\n",
       "<tr><td>Jill Jones MD</td><td>40693 Henry Greens</td><td>NT</td><td>862</td><td>Female</td><td>1194530</td></tr>\n",
       "<tr><td>Lindsay Jimenez</td><td>00653 Davenport C...</td><td>NSW</td><td>2780</td><td>Female</td><td>154128</td></tr>\n",
       "<tr><td>Rebecca Blanchard</td><td>9271 Michael Mano...</td><td>WA</td><td>6355</td><td>Female</td><td>712975</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+-----+--------+------+-----------+\n",
       "|    customer_name|             address|state|postcode|gender|consumer_id|\n",
       "+-----------------+--------------------+-----+--------+------+-----------+\n",
       "| Yolanda Williams|413 Haney Gardens...|   WA|    6935|Female|    1195503|\n",
       "|       Mary Smith|     3764 Amber Oval|  NSW|    2782|Female|     179208|\n",
       "|    Jill Jones MD|  40693 Henry Greens|   NT|     862|Female|    1194530|\n",
       "|  Lindsay Jimenez|00653 Davenport C...|  NSW|    2780|Female|     154128|\n",
       "|Rebecca Blanchard|9271 Michael Mano...|   WA|    6355|Female|     712975|\n",
       "+-----------------+--------------------+-----+--------+------+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumerdf = spark.read.option(\"header\",\"true\").csv(\"../data/tables/tbl_consumer.csv\", sep=\"|\")\n",
    "consumerdf = consumerdf.withColumnRenamed(\"name\",\"customer_name\")\n",
    "consumerdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>company_name</th><th>tags</th><th>merchant_abn</th><th>take_rate</th><th>revenue_band</th></tr>\n",
       "<tr><td>Felis Limited</td><td>furniture, home f...</td><td>10023283211</td><td> 0.18</td><td> e</td></tr>\n",
       "<tr><td>Arcu Ac Orci Corp...</td><td>cable, satellite,...</td><td>10142254217</td><td> 4.22</td><td> b</td></tr>\n",
       "<tr><td>Nunc Sed Company</td><td>jewelry, watch, c...</td><td>10165489824</td><td> 4.40</td><td> b</td></tr>\n",
       "<tr><td>Ultricies Digniss...</td><td>watch, clock, and...</td><td>10187291046</td><td> 3.29</td><td> b</td></tr>\n",
       "<tr><td>Enim Condimentum PC</td><td>music shops - mus...</td><td>10192359162</td><td> 6.33</td><td> a</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+------------+---------+------------+\n",
       "|        company_name|                tags|merchant_abn|take_rate|revenue_band|\n",
       "+--------------------+--------------------+------------+---------+------------+\n",
       "|       Felis Limited|furniture, home f...| 10023283211|     0.18|           e|\n",
       "|Arcu Ac Orci Corp...|cable, satellite,...| 10142254217|     4.22|           b|\n",
       "|    Nunc Sed Company|jewelry, watch, c...| 10165489824|     4.40|           b|\n",
       "|Ultricies Digniss...|watch, clock, and...| 10187291046|     3.29|           b|\n",
       "| Enim Condimentum PC|music shops - mus...| 10192359162|     6.33|           a|\n",
       "+--------------------+--------------------+------------+---------+------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchantdf = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "merchantdf = merchantdf.withColumnRenamed(\"name\",\"company_name\")\n",
    "\n",
    "# Replace all square brackets with round brackets\n",
    "merchantdf = merchantdf.withColumn('tags', regexp_replace('tags', '\\\\[', '\\\\('))\n",
    "merchantdf = merchantdf.withColumn('tags', regexp_replace('tags', '\\\\]', '\\\\)'))\n",
    "\n",
    "# Extract take rate into seperate column\n",
    "merchantdf = merchantdf.withColumn(\"take_rate\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(2))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', 'take rate: ', \n",
    "                                                  ''))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', '\\\\(', ''))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', '\\\\)', ''))\n",
    "\n",
    "# Extract revenue band\n",
    "merchantdf = merchantdf.withColumn(\"revenue_band\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(1))\\\n",
    "                       .withColumn('revenue_band', \n",
    "                                   regexp_replace('revenue_band', '\\\\(', ''))\\\n",
    "                       .withColumn('revenue_band', \n",
    "                                   regexp_replace('revenue_band', '\\\\)', ''))\n",
    "\n",
    "# Extract tags band\n",
    "merchantdf = merchantdf.withColumn(\"tags\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(0))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', '\\\\(', ''))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', '\\\\)', ''))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', ' +', ' '))\\\n",
    "                       .withColumn('tags', \n",
    "                                   lower('tags'))\n",
    "\n",
    "\n",
    "merchantdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8151372\n",
      "8151372\n",
      "\n",
      "\n",
      "8151372 499999\n",
      "8151372\n",
      "\n",
      "\n",
      "8151372 499999\n",
      "8151372\n",
      "\n",
      "\n",
      "8151372 4026\n",
      "7817730\n"
     ]
    }
   ],
   "source": [
    "# Check if transactions combine nicely\n",
    "print(transactiondf1.count() + transactiondf2.count())\n",
    "transactiondf = transactiondf1.union(transactiondf2)\n",
    "print(transactiondf.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check no rows dropped when combining transactions with user\n",
    "print(transactiondf.count(),userdf.count())\n",
    "mergedf = transactiondf.join(userdf, \"user_id\")\n",
    "print(mergedf.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check no rows dropped when combining with consumer\n",
    "print(mergedf.count(), consumerdf.count())\n",
    "mergedf = mergedf.join(consumerdf, \"consumer_id\")\n",
    "print(mergedf.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check no rows dropped when combining with merchant\n",
    "print(mergedf.count(),merchantdf.count())\n",
    "mergedf = mergedf.join(merchantdf, \"merchant_abn\")\n",
    "print(mergedf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that number of rows goes down from 8151372 to 7817730. Since the join was on merchant_abn, this means that either the merchantdf didn't have those merchants on it or the merged df had incorrect merchant_abns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ashahi/generic-buy-now-pay-later-project-group-35/data/curated/mergedf.parquet already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ashahi/generic-buy-now-pay-later-project-group-35/notebooks/1 preprocessing.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ashahi/generic-buy-now-pay-later-project-group-35/notebooks/1%20preprocessing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mergedf\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(\u001b[39m'\u001b[39;49m\u001b[39m../data/curated/mergedf.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ashahi/generic-buy-now-pay-later-project-group-35/data/curated/mergedf.parquet already exists."
     ]
    }
   ],
   "source": [
    "mergedf.write.parquet('../data/curated/mergedf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main websites out there containing australian datasets:\n",
    "- https://explore.data.abs.gov.au/ \n",
    "- https://data.gov.au/\n",
    "- https://researchdata.edu.au/\n",
    "- https://aurin.org.au/\n",
    "\n",
    "Key features to join on: \n",
    "- timestamp\n",
    "- location: state/postcode/SA2 \n",
    "\n",
    "Key model features: \n",
    "- merchant abn (nominal)\n",
    "- datetime\n",
    "- user id: how many users shop at each merchant\n",
    "- dollar value\n",
    "- location\n",
    "- gender\n",
    "- tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA2 Shapefile: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
