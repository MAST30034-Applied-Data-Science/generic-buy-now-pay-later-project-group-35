{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-31T02:22:01.919813Z",
     "start_time": "2022-07-31T02:21:57.283647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 22:20:22 WARN Utils: Your hostname, AryansLaptop resolves to a loopback address: 127.0.1.1; using 172.28.32.93 instead (on interface eth0)\n",
      "22/09/18 22:20:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 22:20:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th></tr>\n",
       "<tr><td>18478</td><td>62191208634</td><td>63.255848959735246</td><td>949a63c8-29f7-4ab...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>2</td><td>15549624934</td><td>130.3505283105634</td><td>6a84c3cf-612a-457...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>18479</td><td>64403598239</td><td>120.15860593212783</td><td>b10dcc33-e53f-425...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>3</td><td>60956456424</td><td>136.6785200286976</td><td>0f09c5a5-784e-447...</td><td>2021-08-20</td></tr>\n",
       "<tr><td>18479</td><td>94493496784</td><td>72.96316578355305</td><td>f6c78c1a-4600-4c5...</td><td>2021-08-20</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|\n",
       "+-------+------------+------------------+--------------------+--------------+\n",
       "|  18478| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|\n",
       "|      2| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|\n",
       "|  18479| 64403598239|120.15860593212783|b10dcc33-e53f-425...|    2021-08-20|\n",
       "|      3| 60956456424| 136.6785200286976|0f09c5a5-784e-447...|    2021-08-20|\n",
       "|  18479| 94493496784| 72.96316578355305|f6c78c1a-4600-4c5...|    2021-08-20|\n",
       "+-------+------------+------------------+--------------------+--------------+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactiondf1 = spark.read.parquet(\"../data/tables/transactions_20210228_20210827_snapshot/\")\n",
    "transactiondf2 = spark.read.parquet(\"../data/tables/transactions_20210828_20220227_snapshot/\")\n",
    "transactiondf3 = spark.read.parquet(\"../data/tables/transactions_20220228_20220828_snapshot/\")\n",
    "transactiondf12 = transactiondf1.union(transactiondf2)\n",
    "transactiondf = transactiondf12.union(transactiondf3)\n",
    "transactiondf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>consumer_id</th></tr>\n",
       "<tr><td>1</td><td>1195503</td></tr>\n",
       "<tr><td>2</td><td>179208</td></tr>\n",
       "<tr><td>3</td><td>1194530</td></tr>\n",
       "<tr><td>4</td><td>154128</td></tr>\n",
       "<tr><td>5</td><td>712975</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------+\n",
       "|user_id|consumer_id|\n",
       "+-------+-----------+\n",
       "|      1|    1195503|\n",
       "|      2|     179208|\n",
       "|      3|    1194530|\n",
       "|      4|     154128|\n",
       "|      5|     712975|\n",
       "+-------+-----------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf = spark.read.parquet(\"../data/tables/consumer_user_details.parquet\")\n",
    "userdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_name</th><th>address</th><th>state</th><th>postcode</th><th>gender</th><th>consumer_id</th></tr>\n",
       "<tr><td>Yolanda Williams</td><td>413 Haney Gardens...</td><td>WA</td><td>6935</td><td>Female</td><td>1195503</td></tr>\n",
       "<tr><td>Mary Smith</td><td>3764 Amber Oval</td><td>NSW</td><td>2782</td><td>Female</td><td>179208</td></tr>\n",
       "<tr><td>Jill Jones MD</td><td>40693 Henry Greens</td><td>NT</td><td>862</td><td>Female</td><td>1194530</td></tr>\n",
       "<tr><td>Lindsay Jimenez</td><td>00653 Davenport C...</td><td>NSW</td><td>2780</td><td>Female</td><td>154128</td></tr>\n",
       "<tr><td>Rebecca Blanchard</td><td>9271 Michael Mano...</td><td>WA</td><td>6355</td><td>Female</td><td>712975</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+-----+--------+------+-----------+\n",
       "|    customer_name|             address|state|postcode|gender|consumer_id|\n",
       "+-----------------+--------------------+-----+--------+------+-----------+\n",
       "| Yolanda Williams|413 Haney Gardens...|   WA|    6935|Female|    1195503|\n",
       "|       Mary Smith|     3764 Amber Oval|  NSW|    2782|Female|     179208|\n",
       "|    Jill Jones MD|  40693 Henry Greens|   NT|     862|Female|    1194530|\n",
       "|  Lindsay Jimenez|00653 Davenport C...|  NSW|    2780|Female|     154128|\n",
       "|Rebecca Blanchard|9271 Michael Mano...|   WA|    6355|Female|     712975|\n",
       "+-----------------+--------------------+-----+--------+------+-----------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumerdf = spark.read.option(\"header\",\"true\").csv(\"../data/tables/tbl_consumer.csv\", sep=\"|\")\n",
    "consumerdf = consumerdf.withColumnRenamed(\"name\",\"customer_name\")\n",
    "consumerdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>company_name</th><th>tags</th><th>merchant_abn</th><th>take_rate</th><th>revenue_band</th></tr>\n",
       "<tr><td>Felis Limited</td><td>furniture, home f...</td><td>10023283211</td><td> 0.18</td><td> e</td></tr>\n",
       "<tr><td>Arcu Ac Orci Corp...</td><td>cable, satellite,...</td><td>10142254217</td><td> 4.22</td><td> b</td></tr>\n",
       "<tr><td>Nunc Sed Company</td><td>jewelry, watch, c...</td><td>10165489824</td><td> 4.40</td><td> b</td></tr>\n",
       "<tr><td>Ultricies Digniss...</td><td>watch, clock, and...</td><td>10187291046</td><td> 3.29</td><td> b</td></tr>\n",
       "<tr><td>Enim Condimentum PC</td><td>music shops - mus...</td><td>10192359162</td><td> 6.33</td><td> a</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+------------+---------+------------+\n",
       "|        company_name|                tags|merchant_abn|take_rate|revenue_band|\n",
       "+--------------------+--------------------+------------+---------+------------+\n",
       "|       Felis Limited|furniture, home f...| 10023283211|     0.18|           e|\n",
       "|Arcu Ac Orci Corp...|cable, satellite,...| 10142254217|     4.22|           b|\n",
       "|    Nunc Sed Company|jewelry, watch, c...| 10165489824|     4.40|           b|\n",
       "|Ultricies Digniss...|watch, clock, and...| 10187291046|     3.29|           b|\n",
       "| Enim Condimentum PC|music shops - mus...| 10192359162|     6.33|           a|\n",
       "+--------------------+--------------------+------------+---------+------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchantdf = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "merchantdf = merchantdf.withColumnRenamed(\"name\",\"company_name\")\n",
    "\n",
    "# Replace all square brackets with round brackets\n",
    "merchantdf = merchantdf.withColumn('tags', regexp_replace('tags', '\\\\[', '\\\\('))\n",
    "merchantdf = merchantdf.withColumn('tags', regexp_replace('tags', '\\\\]', '\\\\)'))\n",
    "\n",
    "# Extract take rate into seperate column\n",
    "merchantdf = merchantdf.withColumn(\"take_rate\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(2))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', 'take rate: ', \n",
    "                                                  ''))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', '\\\\(', ''))\\\n",
    "                       .withColumn('take_rate', \n",
    "                                   regexp_replace('take_rate', '\\\\)', ''))\n",
    "\n",
    "# Extract revenue band\n",
    "merchantdf = merchantdf.withColumn(\"revenue_band\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(1))\\\n",
    "                       .withColumn('revenue_band', \n",
    "                                   regexp_replace('revenue_band', '\\\\(', ''))\\\n",
    "                       .withColumn('revenue_band', \n",
    "                                   regexp_replace('revenue_band', '\\\\)', ''))\n",
    "\n",
    "# Extract tags band\n",
    "merchantdf = merchantdf.withColumn(\"tags\", \n",
    "                                   split(col(\"tags\"), \"\\\\),\").getItem(0))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', '\\\\(', ''))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', '\\\\)', ''))\\\n",
    "                       .withColumn('tags', \n",
    "                                   regexp_replace('tags', ' +', ' '))\\\n",
    "                       .withColumn('tags', \n",
    "                                   lower('tags'))\n",
    "\n",
    "merchantdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14195505 499999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14195505\n",
      "\n",
      "\n",
      "14195505 499999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14195505\n",
      "\n",
      "\n",
      "14195505 4026\n",
      "13614675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check no rows dropped when combining transactions with user\n",
    "print(transactiondf.count(),userdf.count())\n",
    "mergedf = transactiondf.join(userdf, \"user_id\")\n",
    "print(mergedf.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check no rows dropped when combining with consumer\n",
    "print(mergedf.count(), consumerdf.count())\n",
    "mergedf = mergedf.join(consumerdf, \"consumer_id\")\n",
    "print(mergedf.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check no rows dropped when combining with merchant\n",
    "print(mergedf.count(),merchantdf.count())\n",
    "mergedf = mergedf.join(merchantdf, \"merchant_abn\")\n",
    "print(mergedf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that number of rows goes down from 14195505 to 13614675. Since the join was on merchant_abn, this means that either the merchantdf didn't have those merchants on it or the merged df had incorrect merchant_abns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                       (0 + 16) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 22:21:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 96.51% for 14 writers\n",
      "22/09/18 22:21:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 90.08% for 15 writers\n",
      "22/09/18 22:21:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 84.45% for 16 writers\n",
      "22/09/18 22:21:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 90.08% for 15 writers\n",
      "22/09/18 22:21:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 96.51% for 14 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mergedf.write.parquet('../data/curated/mergedf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main websites out there containing australian datasets:\n",
    "- https://explore.data.abs.gov.au/ \n",
    "- https://data.gov.au/\n",
    "- https://researchdata.edu.au/\n",
    "- https://aurin.org.au/\n",
    "\n",
    "Key features to join on: \n",
    "- timestamp\n",
    "- location: state/postcode/SA2 \n",
    "\n",
    "Key model features: \n",
    "- merchant abn (nominal)\n",
    "- datetime\n",
    "- user id: how many users shop at each merchant\n",
    "- dollar value\n",
    "- location\n",
    "- gender\n",
    "- tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA2 Shapefile: https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
